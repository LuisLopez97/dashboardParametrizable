{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import emoji\n",
    "# Permite desplegar el texto completo en Jupyter\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de CSV\n",
    "data = pd.read_csv(\"tweets_es.csv\")\n",
    "#data = df.head(50) # Muestra los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quitar caracteres, link, @, Hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_noURL = data[\"content\"].str.replace('\\w+:\\/\\/\\S+',\"\")\n",
    "data_noUser = data_noURL.str.replace('@(\\w+)',\"\")\n",
    "data_noHashtag = data_noUser.str.replace('#(\\w*)',\"\")\n",
    "data_noEnter = data_noHashtag.str.replace('\\n(w*)',\"\")\n",
    "data_noAmp = data_noEnter.str.replace('&amp',\"\")\n",
    "data_lower=data_noAmp.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = data_lower.str.replace(\"[\"\n",
    "                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                u\"\\U00002702-\\U000027B0\"\n",
    "                u\"\\U000024C2-\\U0001F251\"\n",
    "                u\"\\U0001f926-\\U0001f937\"\n",
    "                u'\\U00010000-\\U0010ffff'\n",
    "                u\"\\u200d\"\n",
    "                u\"\\u2640-\\u2642\"\n",
    "                u\"\\u2600-\\u2B55\"\n",
    "                u\"\\u23cf\"\n",
    "                u\"\\u23e9\"\n",
    "                u\"\\u231a\"\n",
    "                u\"\\u3030\"\n",
    "                u\"\\ufe0f\"\n",
    "    \"]+\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_noPuntuacion = emoji_pattern.str.replace('[^a-zA-Z ]+', '')\n",
    "data_noPuntuacion = data_noPuntuacion.str.replace(\" +\",\" \") # Reducir los espacios a solo 1\n",
    "data_noRepeated = data_noPuntuacion.transform(lambda x: re.sub(r'(.)\\1+', r'\\1\\1', str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\GIYELI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words(\"spanish\")\n",
    "stop_set = set(stop)\n",
    "data_noStopwords = data_noRepeated.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_set)]))\n",
    "#data_noStopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for token in nlp(data_noStopwords):\n",
    "#    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lematizacion (tweet):\n",
    "    #tokens = word_tokenize(tweet)\n",
    "    resultado = []\n",
    "    for token in nlp(tweet):\n",
    "        #print(token.lemma_)\n",
    "        resultado.append(token.lemma_)\n",
    "    return \" \".join(resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    salgo da ms largoo                                                                  \n",
       "1    libraras ayudar menos besos gracias                                                 \n",
       "2    gracias mar                                                                         \n",
       "3    off pensando regalito sinde va sgae van corruptos intento sacar conclusiones intento\n",
       "4    conozco alguien q adicto drama ja ja ja suena d                                     \n",
       "5    rt si amas alguien djalo libre si grita hombre mo                                   \n",
       "6    toca grabacin dl especial navideomari crismas                                       \n",
       "7    hoy asisitir madrid seminario estrategia espaola seguridad organizado faes          \n",
       "8    buen da primero mandar abrazo grande miguel familia hoy podra ser da grandeza humana\n",
       "9    escao listo empezar congreso                                                        \n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_noStopwords.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    salir dar milisegundo largoo                                                       \n",
       "1    librar ayudar menos beso gracia                                                    \n",
       "2    gracia mar                                                                         \n",
       "3    off pensar regalito sinde ir sgae ir corrupto intentar sacar conclusión intentar   \n",
       "4    conocer alguien q adicto drama ja ja ja sonar d                                    \n",
       "5    rt si amar alguien djalo librar si gritar hombre mo                                \n",
       "6    tocar grabacin decilitro especial navideomari crisma                               \n",
       "7    hoy asisitir madrid seminario estrategia espaola seguridad organizar faes          \n",
       "8    bueno dar 1 mandar abrazar grande miguel familia hoy pudrir ser dar grandeza humano\n",
       "9    escao listar empezar congreso                                                      \n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_Lemmatizer= data_noStopwords.apply(lambda x: lematizacion(x))\n",
    "data_Lemmatizer.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_lemmatized</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>salir dar milisegundo largoo</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>librar ayudar menos beso gracia</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gracia mar</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>off pensar regalito sinde ir sgae ir corrupto intentar sacar conclusión intentar</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conocer alguien q adicto drama ja ja ja sonar d</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rt si amar alguien djalo librar si gritar hombre mo</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tocar grabacin decilitro especial navideomari crisma</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hoy asisitir madrid seminario estrategia espaola seguridad organizar faes</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bueno dar 1 mandar abrazar grande miguel familia hoy pudrir ser dar grandeza humano</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>escao listar empezar congreso</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       data_lemmatized  \\\n",
       "0  salir dar milisegundo largoo                                                          \n",
       "1  librar ayudar menos beso gracia                                                       \n",
       "2  gracia mar                                                                            \n",
       "3  off pensar regalito sinde ir sgae ir corrupto intentar sacar conclusión intentar      \n",
       "4  conocer alguien q adicto drama ja ja ja sonar d                                       \n",
       "5  rt si amar alguien djalo librar si gritar hombre mo                                   \n",
       "6  tocar grabacin decilitro especial navideomari crisma                                  \n",
       "7  hoy asisitir madrid seminario estrategia espaola seguridad organizar faes             \n",
       "8  bueno dar 1 mandar abrazar grande miguel familia hoy pudrir ser dar grandeza humano   \n",
       "9  escao listar empezar congreso                                                         \n",
       "\n",
       "  sentiment  \n",
       "0  NONE      \n",
       "1  NEU       \n",
       "2  P         \n",
       "3  N         \n",
       "4  P         \n",
       "5  NONE      \n",
       "6  P         \n",
       "7  NONE      \n",
       "8  P         \n",
       "9  P         "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"data_lemmatized\"] = data_Lemmatizer\n",
    "tweets_limpios = data[['data_lemmatized','sentiment']]\n",
    "tweets_limpios.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_limpios = tweets_limpios.loc[tweets_limpios['sentiment'] != \"NONE\"]\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_limpios[\"sentiment\"] = tweets_limpios[\"sentiment\"].replace(to_replace=[\"P\",\"NEU\",\"N\"], value=[1,0,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_lemmatized</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>librar ayudar menos beso gracia</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gracia mar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>off pensar regalito sinde ir sgae ir corrupto intentar sacar conclusión intentar</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conocer alguien q adicto drama ja ja ja sonar d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tocar grabacin decilitro especial navideomari crisma</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bueno dar 1 mandar abrazar grande miguel familia hoy pudrir ser dar grandeza humano</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>escao listar empezar congreso</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bdas em ira puente si ir dejeis llevar tableta pc luego orbyt momento decir milisegundo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sistema econmico q recortar dinero prestación social reforzar billn mediar d euro banco necesitar repensarse</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>caca d ajustar</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                 data_lemmatized  \\\n",
       "1   librar ayudar menos beso gracia                                                                                \n",
       "2   gracia mar                                                                                                     \n",
       "3   off pensar regalito sinde ir sgae ir corrupto intentar sacar conclusión intentar                               \n",
       "4   conocer alguien q adicto drama ja ja ja sonar d                                                                \n",
       "6   tocar grabacin decilitro especial navideomari crisma                                                           \n",
       "8   bueno dar 1 mandar abrazar grande miguel familia hoy pudrir ser dar grandeza humano                            \n",
       "9   escao listar empezar congreso                                                                                  \n",
       "10  bdas em ira puente si ir dejeis llevar tableta pc luego orbyt momento decir milisegundo                        \n",
       "11  sistema econmico q recortar dinero prestación social reforzar billn mediar d euro banco necesitar repensarse   \n",
       "12  caca d ajustar                                                                                                 \n",
       "\n",
       "    sentiment  \n",
       "1   0          \n",
       "2   1          \n",
       "3  -1          \n",
       "4   1          \n",
       "6   1          \n",
       "8   1          \n",
       "9   1          \n",
       "10  1          \n",
       "11  1          \n",
       "12 -1          "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_limpios.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_limpios.to_csv('data_lemmatized_es.csv', index=None, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    3663\n",
       "-1    3153\n",
       " 0    1063\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_limpios[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_df = pd.read_csv(\"tweets_es.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.loc[test_df['sentiment'] != \"NONE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"sentiment\"] = test_df[\"sentiment\"].replace(to_replace=[\"P\", \"NEU\", \"N\"], value=[1, 0, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetid</th>\n",
       "      <th>user</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>lang</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>142389933619945473</td>\n",
       "      <td>CarmendelRiego</td>\n",
       "      <td>@PauladeLasHeras No te libraras de ayudar me/n...</td>\n",
       "      <td>2011-12-02T00:49:40</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>142391947707940864</td>\n",
       "      <td>CarmendelRiego</td>\n",
       "      <td>@marodriguezb Gracias MAR</td>\n",
       "      <td>2011-12-02T00:57:40</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>142416095012339712</td>\n",
       "      <td>mgilguerrero</td>\n",
       "      <td>Off pensando en el regalito Sinde, la que se v...</td>\n",
       "      <td>2011-12-02T02:33:37</td>\n",
       "      <td>es</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142422495721562112</td>\n",
       "      <td>paurubio</td>\n",
       "      <td>Conozco a alguien q es adicto al drama! Ja ja ...</td>\n",
       "      <td>2011-12-02T02:59:03</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>142483342040907776</td>\n",
       "      <td>Carlos_Latre</td>\n",
       "      <td>Toca @crackoviadeTV3 . Grabación dl especial N...</td>\n",
       "      <td>2011-12-02T07:00:50</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>142494476051562496</td>\n",
       "      <td>nacho_uriarte</td>\n",
       "      <td>Buen día todos! Lo primero mandar un abrazo gr...</td>\n",
       "      <td>2011-12-02T07:45:05</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>142496796416016384</td>\n",
       "      <td>JuanraLucas</td>\n",
       "      <td>Desde el escaño. Todo listo para empezar #endi...</td>\n",
       "      <td>2011-12-02T07:54:19</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>142497735814287360</td>\n",
       "      <td>pedroj_ramirez</td>\n",
       "      <td>Bdías. EM no se ira de puente. Si vosotros os ...</td>\n",
       "      <td>2011-12-02T07:58:02</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>142499355360903168</td>\n",
       "      <td>sevillajordi</td>\n",
       "      <td>Un sistema económico q recorta dinero para pre...</td>\n",
       "      <td>2011-12-02T08:04:28</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>142504935853006848</td>\n",
       "      <td>Carlos_Latre</td>\n",
       "      <td>#programascambiados caca d ajuste</td>\n",
       "      <td>2011-12-02T08:26:38</td>\n",
       "      <td>es</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tweetid            user  \\\n",
       "1   142389933619945473  CarmendelRiego   \n",
       "2   142391947707940864  CarmendelRiego   \n",
       "3   142416095012339712    mgilguerrero   \n",
       "4   142422495721562112        paurubio   \n",
       "6   142483342040907776    Carlos_Latre   \n",
       "8   142494476051562496   nacho_uriarte   \n",
       "9   142496796416016384     JuanraLucas   \n",
       "10  142497735814287360  pedroj_ramirez   \n",
       "11  142499355360903168    sevillajordi   \n",
       "12  142504935853006848    Carlos_Latre   \n",
       "\n",
       "                                              content                 date  \\\n",
       "1   @PauladeLasHeras No te libraras de ayudar me/n...  2011-12-02T00:49:40   \n",
       "2                           @marodriguezb Gracias MAR  2011-12-02T00:57:40   \n",
       "3   Off pensando en el regalito Sinde, la que se v...  2011-12-02T02:33:37   \n",
       "4   Conozco a alguien q es adicto al drama! Ja ja ...  2011-12-02T02:59:03   \n",
       "6   Toca @crackoviadeTV3 . Grabación dl especial N...  2011-12-02T07:00:50   \n",
       "8   Buen día todos! Lo primero mandar un abrazo gr...  2011-12-02T07:45:05   \n",
       "9   Desde el escaño. Todo listo para empezar #endi...  2011-12-02T07:54:19   \n",
       "10  Bdías. EM no se ira de puente. Si vosotros os ...  2011-12-02T07:58:02   \n",
       "11  Un sistema económico q recorta dinero para pre...  2011-12-02T08:04:28   \n",
       "12                  #programascambiados caca d ajuste  2011-12-02T08:26:38   \n",
       "\n",
       "   lang  sentiment  \n",
       "1    es          0  \n",
       "2    es          1  \n",
       "3    es         -1  \n",
       "4    es          1  \n",
       "6    es          1  \n",
       "8    es          1  \n",
       "9    es          1  \n",
       "10   es          1  \n",
       "11   es          1  \n",
       "12   es         -1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('tweets_es.csv', index=None, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
