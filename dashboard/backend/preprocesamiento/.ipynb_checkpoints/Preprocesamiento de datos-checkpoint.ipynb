{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento con Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el preprocesamiento de texto utilizaremos las librerías de Python:\n",
    "- **Numpy**: para el procesamiento de las operaciones en los Dataframes y Series de Pandas\n",
    "- **Pandas**: para la manipulación de los datos\n",
    "- **NLTK**: para el procesamiento de texto por medio de las StopWords, Stemming, Lemmatization y POS tag\n",
    "- **re**: filtrar datos con para expresiones regulares "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de datos con Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importacion de librerias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "# Permite desplegar el texto completo en Jupyter\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de CSV\n",
    "df = pd.read_csv(\"Tweets_pg_prepared.csv\")\n",
    "data = df.tail() # Muestra los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14635    @AmericanAir thank you we got on a different flight to Chicago.                                                                                       \n",
       "14636    @AmericanAir leaving over 20 minutes Late Flight. No warnings or communication until we were 15 minutes Late Flight. That's called shitty customer svc\n",
       "14637    @AmericanAir Please bring American Airlines to #BlackBerry10                                                                                          \n",
       "14638    @AmericanAir you have my money, you change my flight, and don't answer your phones! Any other suggestions so I can make my commitment??               \n",
       "14639    @AmericanAir we have 8 ppl so we need 2 know how many seats are on the next flight. Plz put us on standby for 4 people on the next flight?            \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"text\"] # Mostrar los datos de la columna \"text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertir mayúsculas a minúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14635    @americanair thank you we got on a different flight to chicago.                                                                                       \n",
       "14636    @americanair leaving over 20 minutes late flight. no warnings or communication until we were 15 minutes late flight. that's called shitty customer svc\n",
       "14637    @americanair please bring american airlines to #blackberry10                                                                                          \n",
       "14638    @americanair you have my money, you change my flight, and don't answer your phones! any other suggestions so i can make my commitment??               \n",
       "14639    @americanair we have 8 ppl so we need 2 know how many seats are on the next flight. plz put us on standby for 4 people on the next flight?            \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lower = data[\"text\"].str.lower() # Convertir todo el texto de la columna \"text\" a minusculas\n",
    "data_lower # Mostrar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remover URLs (Regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicación de Regex\n",
    "1. **(http|https|ftp)**: Detectar si empieza con alguno de estos protocolos\n",
    "2. **://**: Seguido de un \"://\"\n",
    "3. **[a-zA-Z0-9\\\\./]**: E inmediatamente empieza una palabra seguido de un punto (.) una o mas veces (de esta manera se incluye el (.com y variantes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14635    @americanair thank you we got on a different flight to chicago.                                                                                       \n",
       "14636    @americanair leaving over 20 minutes late flight. no warnings or communication until we were 15 minutes late flight. that's called shitty customer svc\n",
       "14637    @americanair please bring american airlines to #blackberry10                                                                                          \n",
       "14638    @americanair you have my money, you change my flight, and don't answer your phones! any other suggestions so i can make my commitment??               \n",
       "14639    @americanair we have 8 ppl so we need 2 know how many seats are on the next flight. plz put us on standby for 4 people on the next flight?            \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_noURL = data_lower.str.replace('(http|https|ftp)://[a-zA-Z0-9\\\\./]+',\"\")\n",
    "data_noURL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remover referencias (@Usernames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicación regex\n",
    "1. **@**: Si empieza con arroba (@)\n",
    "2. **(\\w+)**: y le sigue una o más palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14635     thank you we got on a different flight to chicago.                                                                                       \n",
       "14636     leaving over 20 minutes late flight. no warnings or communication until we were 15 minutes late flight. that's called shitty customer svc\n",
       "14637     please bring american airlines to #blackberry10                                                                                          \n",
       "14638     you have my money, you change my flight, and don't answer your phones! any other suggestions so i can make my commitment??               \n",
       "14639     we have 8 ppl so we need 2 know how many seats are on the next flight. plz put us on standby for 4 people on the next flight?            \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_noUser = data_noURL.str.replace('@(\\w+)',\"\")\n",
    "data_noUser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remover hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicación regex\n",
    "1. **#**: Si empieza con arroba (#)\n",
    "2. **(\\w+)**: y le sigue una o más palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14635     thank you we got on a different flight to chicago.                                                                                       \n",
       "14636     leaving over 20 minutes late flight. no warnings or communication until we were 15 minutes late flight. that's called shitty customer svc\n",
       "14637     please bring american airlines to                                                                                                        \n",
       "14638     you have my money, you change my flight, and don't answer your phones! any other suggestions so i can make my commitment??               \n",
       "14639     we have 8 ppl so we need 2 know how many seats are on the next flight. plz put us on standby for 4 people on the next flight?            \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_noHashtag = data_noUser.str.replace('#(\\w+)',\"\")\n",
    "data_noHashtag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remover emoticones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTA: Falta averiguar si algún analizador de sentimientos le sirven los emoticones, o tal vez se puedan traducir los emojis por alguna palabra que exprese su sentimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14635     thank you we got on a different flight to chicago.                                                                                       \n",
       "14636     leaving over 20 minutes late flight. no warnings or communication until we were 15 minutes late flight. that's called shitty customer svc\n",
       "14637     please bring american airlines to                                                                                                        \n",
       "14638     you have my money, you change my flight, and don't answer your phones! any other suggestions so i can make my commitment??               \n",
       "14639     we have 8 ppl so we need 2 know how many seats are on the next flight. plz put us on standby for 4 people on the next flight?            \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_noEmoji = data_noHashtag.str.replace(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticones\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # simbolos & pictografos\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # simbolos de transporte y mapas\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # banderas (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", \"\")\n",
    "data_noEmoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remover StopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Son palabras que no aportan valor al analizar sentimientos, en Inglés serían palabras como \"are, you, have, etc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14635    thank got different flight chicago.                                                                             \n",
       "14636    leaving 20 minutes late flight. warnings communication 15 minutes late flight. that's called shitty customer svc\n",
       "14637    please bring american airlines                                                                                  \n",
       "14638    money, change flight, answer phones! suggestions make commitment??                                              \n",
       "14639    8 ppl need 2 know many seats next flight. plz put us standby 4 people next flight?                              \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "#from nltk.corpus import stopwords\n",
    "stop = stopwords.words(\"english\")\n",
    "data_noStopwords = data_noEmoji.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "data_noStopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretación de Slang (abreviaturas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scrapping de los acronimos de Netlingo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# import requests, json\n",
    "# resp = requests.get(\"http://www.netlingo.com/acronyms.php\")\n",
    "# soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "# slangdict = {}\n",
    "# key = \"\"\n",
    "# value = \"\"\n",
    "# for div in soup.findAll('div', attrs={'class':'list_box3'}):\n",
    "#     for li in div.findAll('li'):\n",
    "#         for a in li.findAll('a'):\n",
    "#             key = a.text\n",
    "#         value = li.text.split(key)[1]\n",
    "#         slangdict[key] = value\n",
    "# with open('myslang.json','w') as find:\n",
    "#     json.dump(slangdict, find, indent = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leer el archivo de Slang en JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "!               I have a comment                             \n",
       "#FF             Follow Friday                                \n",
       "(U)             it means arms around you, hug for you        \n",
       "*$              Starbucks                                    \n",
       "**//            it means wink wink, nudge nudge              \n",
       ",!!!!           Talk to the hand                             \n",
       "/R/             Requesting                                   \n",
       "02              Your (or my) two cents worth, also seen as m.\n",
       "10Q             Thank you                                    \n",
       "1174            Nude club                                    \n",
       "121             One to one                                   \n",
       "123             it means I agree                             \n",
       "1337            Elite -or- leet -or- L337                    \n",
       "14              it refers to the fourteen words              \n",
       "142n8ly         Unfortunately                                \n",
       "143             I love you                                   \n",
       "1432            I Love You Too                               \n",
       "14AA41          One for All and All for One                  \n",
       "182             I hate you                                   \n",
       "187             it means murder/ homicide                    \n",
       "19              0 hand                                       \n",
       "1daful          it means wonderful                           \n",
       "1V4             One Vs. Four                                 \n",
       "2               it means to, too, two                        \n",
       "20              Location                                     \n",
       "24/7            Twenty Four Seven, as in all the time        \n",
       "2b              To be                                        \n",
       "2B or not 2B    To Be Or Not To Be                           \n",
       "2b@             To Be At                                     \n",
       "2BZ4UQT         Too Busy For You Cutey                       \n",
       "                         ...                                 \n",
       "YRO             Your Rights Online                           \n",
       "YRYOCC          You're Running on Your Own Cuckoo Clock      \n",
       "YS              You Stinker                                  \n",
       "YSAN            You're Such A Nerd                           \n",
       "ysdiw8          why should i wait?                           \n",
       "YSIC            Why Should I Care?                           \n",
       "YSK             You Should Know                              \n",
       "YSVW            You're So Very Welcome                       \n",
       "YSYD            Yeah, Sure You Do                            \n",
       "YTB             You're The Best                              \n",
       "YTRNW           Yeah That's Right, Now What?                 \n",
       "YTS             You're Too Slow -or- Twitter Search          \n",
       "YTTM            You Talk Too Much                            \n",
       "YTTT            You Telling The Truth?                       \n",
       "YTTWROOMM       You Took The Words Right Out Of My Mouth     \n",
       "YUMPI           Young Upwardly Mobile Professional Idiot     \n",
       "YUPPIES         Young Urban Professionals                    \n",
       "YVW             You're Very Welcome                          \n",
       "YW              You're Welcome                               \n",
       "YWIA            You're Welcome In Advance                    \n",
       "YY4U            Too Wise For You                             \n",
       "YYSSW           Yeah Yeah Sure Sure Whatever                 \n",
       "Z               it means said                                \n",
       "zerg            To gang up on someone                        \n",
       "ZMG or ZOMG     Oh My God                                    \n",
       "ZZZ             Sleeping, Bored, Tired                       \n",
       "\\M/             Heavy Metal Music                            \n",
       "^5              High Five                                    \n",
       "^RUP^           Read Up Please                               \n",
       "^URS            Up Yours                                     \n",
       "Length: 2864, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slang = pd.read_json(\"myslang.json\", typ = \"series\")\n",
    "# slang.to_frame('count') #para convertir a DataFrame\n",
    "slang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import re\n",
    "def translator(user_string):\n",
    "    user_string = user_string.split(\" \")\n",
    "    j = 0\n",
    "    slang = pd.read_json(\"myslang.json\", typ = \"series\")\n",
    "    for _str in user_string:\n",
    "        # Removiendo carácteres especiales\n",
    "        _str = re.sub('[^a-zA-Z0-9-_.]', '', _str)\n",
    "        for k in slang.keys():\n",
    "        # Checa si las palabras seleccionadas coinciden con las abreviaturas en el archivo de \"slang.txt\"\n",
    "            if _str.upper() == k.upper():\n",
    "                # Si encuentra una coincidencia, la reemplaza con su respectiva traducción\n",
    "                user_string[j] = slang[k].lower()\n",
    "    j = j + 1\n",
    "    # Retorna la cadena corregida\n",
    "    return ' '.join(user_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_noSlang = data_noStopwords.apply(lambda x: translator(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Si los slangs hubieran estado en un TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import csv, rea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def translator(user_string):\n",
    "#     user_string = user_string.split(\" \")\n",
    "#     j = 0\n",
    "#     for _str in user_string:\n",
    "#         # Archivo con las abreviaturas y su traducción\n",
    "#         fileName = \"slang.txt\"\n",
    "#         # Modo de Acceso al archivo (lectura)\n",
    "#         accessMode = \"r\"\n",
    "#         with open(fileName, accessMode) as myCSVfile:\n",
    "#             # Leer un archivo como un CSV con delimitador como \"=\", para que la abreviacion sea guardada en row[0] y las frases en row[1]\n",
    "#             dataFromFile = csv.reader(myCSVfile, delimiter=\"=\")\n",
    "#             # Removiendo carácteres especiales\n",
    "#             _str = re.sub('[^a-zA-Z0-9-_.]', '', _str)\n",
    "#             for row in dataFromFile:\n",
    "#                 # Checa si las palabras seleccionadas coinciden con las abreviaturas en el archivo de \"slang.txt\"\n",
    "#                 if _str.upper() == row[0]:\n",
    "#                     # Si encuentra una coincidencia, la reemplaza con su respectiva traducción\n",
    "#                     user_string[j] = row[1].lower()\n",
    "#             myCSVfile.close()\n",
    "#         j = j + 1\n",
    "#     # Retorna la cadena corregida\n",
    "#     return ' '.join(user_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_noSlang = data_noStopwords.apply(lambda x: translator(x))\n",
    "#data_noSlang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducción de carácteres repetidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como \"haaapppyyyy\" a \"haappyy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14635    thank got different flight chicago.                                                                              \n",
       "14636    location 20 minutes late flight. warnings communication 15 minutes late flight. that's called shitty customer svc\n",
       "14637    please bring american airlines                                                                                   \n",
       "14638    money, change flight, answer phones! suggestions make commitment??                                               \n",
       "14639    for, four ppl need 2 know many seats next flight. plz put us standby 4 people next flight?                       \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_noRepeated = data_noSlang.transform(lambda x: re.sub(r'(.)\\1+', r'\\1\\1', x))\n",
    "data_noRepeated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming (volver a las palabras a su respectiva palabra raiz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen diferentes tipos de Stemmers, para el lenguaje Inglés,  podemos encontrar 2 de las más populares en la librería NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es conocido por su simplicidad y velocidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14635    thank got differ flight chicago.                                                          \n",
       "14636    locat 20 minut late flight. warn commun 15 minut late flight. that' call shitti custom svc\n",
       "14637    pleas bring american airlin                                                               \n",
       "14638    money, chang flight, answer phones! suggest make commitment??                             \n",
       "14639    for, four ppl need 2 know mani seat next flight. plz put us standbi 4 peopl next flight?  \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "data_PorterStemming = data_noRepeated.apply(lambda x: ' '.join([ps.stem(word) for word in x.split()]))\n",
    "data_PorterStemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LancasterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es conocido por ser simple, pero tambien en ser muy duro al stemmizar, ya que realiza iteraciones y podría ocurrir una sobre-stemmización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14635    thank got diff flight chicago.                                                         \n",
       "14636    loc 20 minut lat flight. warn commun 15 minut lat flight. that's cal shitty custom svc \n",
       "14637    pleas bring am airlin                                                                  \n",
       "14638    money, chang flight, answ phones! suggest mak commitment??                             \n",
       "14639    for, four ppl nee 2 know many seat next flight. plz put us standby 4 peopl next flight?\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = LancasterStemmer()\n",
    "data_LancasterStemming = data_noRepeated.apply(lambda x: ' '.join([ls.stem(word) for word in x.split()]))\n",
    "data_LancasterStemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, ambos stemmers por si solos devuelven la cadena completa como si se tratara de una palabra:\n",
    "\n",
    "' plu ad commerc experience.. tacky.'\n",
    "\n",
    "Cuando debería ser:\n",
    "\n",
    "['plu' 'ad' 'commerc' 'experience' 'tacky']\n",
    "\n",
    "Para lograr ello realizamos una \"Tokenización\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aleja_000\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14635    [thank, got, differ, flight, chicago]                                                           \n",
       "14636    [locat, 20, minut, late, flight, commun, 15, minut, late, flight, 's, call, shitti, custom, svc]\n",
       "14637    [pleas, bring, american, airlin]                                                                \n",
       "14638    [money, flight, phone, make, commit]                                                            \n",
       "14639    [for, ppl, need, 2, know, mani, seat, next, flight, put, us, standbi, 4, peopl, next, flight]   \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "#from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "def stemOracion(oracion):\n",
    "    token_words = word_tokenize(oracion)\n",
    "    token_words\n",
    "    stem_sentence = []\n",
    "    punctuations = \"?:!.,;\"\n",
    "    for word in token_words:\n",
    "        if word in punctuations:\n",
    "            token_words.remove(word)\n",
    "            continue\n",
    "        stem_sentence.append(ps.stem(word))\n",
    "    return stem_sentence\n",
    "\n",
    "\n",
    "porter_stemmer_tokenized = data_noRepeated.apply(lambda x: stemOracion(x))\n",
    "porter_stemmer_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aleja_000\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14635    [thank, got, diff, flight, chicago]                                                         \n",
       "14636    [loc, 20, minut, lat, flight, commun, 15, minut, lat, flight, 's, cal, shitty, custom, svc] \n",
       "14637    [pleas, bring, am, airlin]                                                                  \n",
       "14638    [money, flight, phon, mak, commit]                                                          \n",
       "14639    [for, ppl, nee, 2, know, many, seat, next, flight, put, us, standby, 4, peopl, next, flight]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "#from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "def stemOracion(oracion):\n",
    "    token_words = word_tokenize(oracion)\n",
    "    token_words\n",
    "    stem_sentence = []\n",
    "    punctuations = \"?:!.,;\"\n",
    "    for word in token_words:\n",
    "        if word in punctuations:\n",
    "            token_words.remove(word)\n",
    "            continue\n",
    "        stem_sentence.append(ls.stem(word))\n",
    "    return stem_sentence\n",
    "\n",
    "\n",
    "porter_stemmer_tokenized = data_noRepeated.apply(lambda x: stemOracion(x))\n",
    "porter_stemmer_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization (es el Stemming pero con otro proceso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se desarrollara el Lemmatization para ver si con este proceso se obtienen mejores resultados. El siguiente código tambien incluye la Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aleja_000\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14635    [thank, get, different, flight, chicago]                                                                        \n",
       "14636    [location, 20, minutes, late, flight, communication, 15, minutes, late, flight, 's, call, shitty, customer, svc]\n",
       "14637    [please, bring, american, airlines]                                                                             \n",
       "14638    [money, flight, phone, make, commitment]                                                                        \n",
       "14639    [for, ppl, need, 2, know, many, seat, next, flight, put, us, standby, 4, people, next, flight]                  \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fuente: https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def lemmatization(oracion):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    punctuations = \"?:!.,;$\\\"\\'\\´\\``\\”\\“\\''\"\n",
    "    resultado = []\n",
    "    sentence_words = nltk.word_tokenize(oracion)\n",
    "    for word in sentence_words:\n",
    "        if word in punctuations:\n",
    "            sentence_words.remove(word)\n",
    "            continue\n",
    "        resultado.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "    return resultado\n",
    "\n",
    "data_lemmatized = data_noRepeated.apply(lambda x: lemmatization(x))\n",
    "data_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Of Speech Tagging (POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sirve para etiquetar cada palabra en la oración como verbo, sustantivo o pronombre, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\aleja_000\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14635    [(thank, NN), (got, VBD), (different, JJ), (flight, NN), (chicago, NN), (., .)]                                                                                                                                                                              \n",
       "14636    [(location, NN), (20, CD), (minutes, NNS), (late, JJ), (flight, NN), (., .), (warnings, NNS), (communication, NN), (15, CD), (minutes, NNS), (late, JJ), (flight, NN), (., .), (that, DT), ('s, VBZ), (called, VBN), (shitty, JJ), (customer, NN), (svc, NN)]\n",
       "14637    [(please, NN), (bring, VB), (american, JJ), (airlines, NNS)]                                                                                                                                                                                                 \n",
       "14638    [(money, NN), (,, ,), (change, NN), (flight, NN), (,, ,), (answer, JJR), (phones, NNS), (!, .), (suggestions, NNS), (make, VBP), (commitment, NN), (?, .), (?, .)]                                                                                           \n",
       "14639    [(for, IN), (,, ,), (four, CD), (ppl, NN), (need, VBP), (2, CD), (know, VBP), (many, JJ), (seats, NNS), (next, JJ), (flight, NN), (., .), (plz, NN), (put, VBD), (us, PRP), (standby, VB), (4, CD), (people, NNS), (next, JJ), (flight, NN), (?, .)]         \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fuente: https://towardsdatascience.com/basic-data-cleaning-engineering-session-twitter-sentiment-data-95e5bd2869ec\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "data_POS = data_noRepeated.apply(lambda x: nltk.pos_tag(nltk.word_tokenize(x)))\n",
    "data_POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"En este caso, puedes definir una característica por cada palabra, indicando si el documento contiene esa palabra. Para ponerle un número limite de características que el clasificador necesita procesar, se empieza por construir una lista de las 2000 palabras mas frecuentes en el corpus en general\"\n",
    "\n",
    "Fuente: http://www.nltk.org/book/ch06.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero necesitamos hacer una lista de todas las palabras (**Bag of Words**)\n",
    "\n",
    "Como tengo un objeto de tipo \"Series\" de pandas, primero necesito convertirlo a una lista, para crear así, una **lista de listas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = data_lemmatized.tolist()\n",
    "# data_lemmatized_prepared = data_lemmatized.apply(lambda x: ' '.join(x))\n",
    "# data_lemmatized_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y crear una lista con todas las palabras, iterando la lista de listas y adjuntandolas a una nueva lista unidimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank',\n",
       " 'get',\n",
       " 'different',\n",
       " 'flight',\n",
       " 'chicago',\n",
       " 'location',\n",
       " '20',\n",
       " 'minutes',\n",
       " 'late',\n",
       " 'flight',\n",
       " 'communication',\n",
       " '15',\n",
       " 'minutes',\n",
       " 'late',\n",
       " 'flight',\n",
       " \"'s\",\n",
       " 'call',\n",
       " 'shitty',\n",
       " 'customer',\n",
       " 'svc',\n",
       " 'please',\n",
       " 'bring',\n",
       " 'american',\n",
       " 'airlines',\n",
       " 'money',\n",
       " 'flight',\n",
       " 'phone',\n",
       " 'make',\n",
       " 'commitment',\n",
       " 'for',\n",
       " 'ppl',\n",
       " 'need',\n",
       " '2',\n",
       " 'know',\n",
       " 'many',\n",
       " 'seat',\n",
       " 'next',\n",
       " 'flight',\n",
       " 'put',\n",
       " 'us',\n",
       " 'standby',\n",
       " '4',\n",
       " 'people',\n",
       " 'next',\n",
       " 'flight']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el feature extractor\n",
    "\n",
    "# Utilizar FreqDist para encontrar las palabras más utilizadas en todos los documentos\n",
    "all_words_freq = nltk.FreqDist(all_words)\n",
    "\n",
    "# Y tomar los primeros 2000\n",
    "word_features = list(all_words_freq)[:2000]\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank',\n",
       " 'get',\n",
       " 'different',\n",
       " 'flight',\n",
       " 'chicago',\n",
       " 'location',\n",
       " '20',\n",
       " 'minutes',\n",
       " 'late',\n",
       " 'communication',\n",
       " '15',\n",
       " \"'s\",\n",
       " 'call',\n",
       " 'shitty',\n",
       " 'customer',\n",
       " 'svc',\n",
       " 'please',\n",
       " 'bring',\n",
       " 'american',\n",
       " 'airlines',\n",
       " 'money',\n",
       " 'phone',\n",
       " 'make',\n",
       " 'commitment',\n",
       " 'for',\n",
       " 'ppl',\n",
       " 'need',\n",
       " '2',\n",
       " 'know',\n",
       " 'many',\n",
       " 'seat',\n",
       " 'next',\n",
       " 'put',\n",
       " 'us',\n",
       " 'standby',\n",
       " '4',\n",
       " 'people']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecutando la funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contains(thank)': True,\n",
       " 'contains(get)': True,\n",
       " 'contains(different)': True,\n",
       " 'contains(flight)': True,\n",
       " 'contains(chicago)': True,\n",
       " 'contains(location)': True,\n",
       " 'contains(20)': True,\n",
       " 'contains(minutes)': True,\n",
       " 'contains(late)': True,\n",
       " 'contains(communication)': True,\n",
       " 'contains(15)': True,\n",
       " \"contains('s)\": True,\n",
       " 'contains(call)': True,\n",
       " 'contains(shitty)': True,\n",
       " 'contains(customer)': True,\n",
       " 'contains(svc)': True,\n",
       " 'contains(please)': True,\n",
       " 'contains(bring)': True,\n",
       " 'contains(american)': True,\n",
       " 'contains(airlines)': True,\n",
       " 'contains(money)': True,\n",
       " 'contains(phone)': True,\n",
       " 'contains(make)': True,\n",
       " 'contains(commitment)': True,\n",
       " 'contains(for)': True,\n",
       " 'contains(ppl)': True,\n",
       " 'contains(need)': True,\n",
       " 'contains(2)': True,\n",
       " 'contains(know)': True,\n",
       " 'contains(many)': True,\n",
       " 'contains(seat)': True,\n",
       " 'contains(next)': True,\n",
       " 'contains(put)': True,\n",
       " 'contains(us)': True,\n",
       " 'contains(standby)': True,\n",
       " 'contains(4)': True,\n",
       " 'contains(people)': True}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_features(word_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivoteo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora necesitamos crear una estructura en donde las filas sean los documentos y las columnas cada palabra en ese documento con su respectiva clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertir los valores de Sentiment a 0 y 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14635    1\n",
       "14636   -1\n",
       "14637    0\n",
       "14638   -1\n",
       "14639    0\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment = data[\"airline_sentiment\"].replace(to_replace=[\"positive\",\"neutral\",\"negative\"], value=[1,0,-1])\n",
    "sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminar Columnas inecesarias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta manera solo conservamos las columnas que queremos tener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_lemmatized</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>[thank, get, different, flight, chicago]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>[location, 20, minutes, late, flight, communication, 15, minutes, late, flight, 's, call, shitty, customer, svc]</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>[please, bring, american, airlines]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>[money, flight, phone, make, commitment]</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14639</th>\n",
       "      <td>[for, ppl, need, 2, know, many, seat, next, flight, put, us, standby, 4, people, next, flight]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                        data_lemmatized  \\\n",
       "14635  [thank, get, different, flight, chicago]                                                                           \n",
       "14636  [location, 20, minutes, late, flight, communication, 15, minutes, late, flight, 's, call, shitty, customer, svc]   \n",
       "14637  [please, bring, american, airlines]                                                                                \n",
       "14638  [money, flight, phone, make, commitment]                                                                           \n",
       "14639  [for, ppl, need, 2, know, many, seat, next, flight, put, us, standby, 4, people, next, flight]                     \n",
       "\n",
       "       sentiment  \n",
       "14635  1          \n",
       "14636 -1          \n",
       "14637  0          \n",
       "14638 -1          \n",
       "14639  0          "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_sentiment = pd.DataFrame(dict(data_lemmatized = data_lemmatized, sentiment = sentiment))\n",
    "bag_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = bag_sentiment.values.tolist()\n",
    "feature_sets = [(document_features(d), c) for (d,c) in bag]\n",
    "train_set, test_set = feature_sets[100:], feature_sets[:100]\n",
    "#classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'contains(thank)': False, 'contains(get)': False, 'contains(different)': False, 'contains(flight)': True, 'contains(chicago)': False, 'contains(location)': False, 'contains(20)': False, 'contains(minutes)': False, 'contains(late)': False, 'contains(communication)': False, 'contains(15)': False, 'contains('s)': False, 'contains(call)': False, 'contains(shitty)': False, 'contains(customer)': False, 'contains(svc)': False, 'contains(please)': False, 'contains(bring)': False, 'contains(american)': False, 'contains(airlines)': False, 'contains(money)': True, 'contains(phone)': True, 'contains(make)': True, 'contains(commitment)': True, 'contains(for)': False, 'contains(ppl)': False, 'contains(need)': False, 'contains(2)': False, 'contains(know)': False, 'contains(many)': False, 'contains(seat)': False, 'contains(next)': False, 'contains(put)': False, 'contains(us)': False, 'contains(standby)': False, 'contains(4)': False, 'contains(people)': False}</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'contains(thank)': False, 'contains(get)': False, 'contains(different)': False, 'contains(flight)': True, 'contains(chicago)': False, 'contains(location)': False, 'contains(20)': False, 'contains(minutes)': False, 'contains(late)': False, 'contains(communication)': False, 'contains(15)': False, 'contains('s)': False, 'contains(call)': False, 'contains(shitty)': False, 'contains(customer)': False, 'contains(svc)': False, 'contains(please)': False, 'contains(bring)': False, 'contains(american)': False, 'contains(airlines)': False, 'contains(money)': False, 'contains(phone)': False, 'contains(make)': False, 'contains(commitment)': False, 'contains(for)': True, 'contains(ppl)': True, 'contains(need)': True, 'contains(2)': True, 'contains(know)': True, 'contains(many)': True, 'contains(seat)': True, 'contains(next)': True, 'contains(put)': True, 'contains(us)': True, 'contains(standby)': True, 'contains(4)': True, 'contains(people)': True}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                0  \\\n",
       "0  {'contains(thank)': False, 'contains(get)': False, 'contains(different)': False, 'contains(flight)': True, 'contains(chicago)': False, 'contains(location)': False, 'contains(20)': False, 'contains(minutes)': False, 'contains(late)': False, 'contains(communication)': False, 'contains(15)': False, 'contains('s)': False, 'contains(call)': False, 'contains(shitty)': False, 'contains(customer)': False, 'contains(svc)': False, 'contains(please)': False, 'contains(bring)': False, 'contains(american)': False, 'contains(airlines)': False, 'contains(money)': True, 'contains(phone)': True, 'contains(make)': True, 'contains(commitment)': True, 'contains(for)': False, 'contains(ppl)': False, 'contains(need)': False, 'contains(2)': False, 'contains(know)': False, 'contains(many)': False, 'contains(seat)': False, 'contains(next)': False, 'contains(put)': False, 'contains(us)': False, 'contains(standby)': False, 'contains(4)': False, 'contains(people)': False}   \n",
       "1  {'contains(thank)': False, 'contains(get)': False, 'contains(different)': False, 'contains(flight)': True, 'contains(chicago)': False, 'contains(location)': False, 'contains(20)': False, 'contains(minutes)': False, 'contains(late)': False, 'contains(communication)': False, 'contains(15)': False, 'contains('s)': False, 'contains(call)': False, 'contains(shitty)': False, 'contains(customer)': False, 'contains(svc)': False, 'contains(please)': False, 'contains(bring)': False, 'contains(american)': False, 'contains(airlines)': False, 'contains(money)': False, 'contains(phone)': False, 'contains(make)': False, 'contains(commitment)': False, 'contains(for)': True, 'contains(ppl)': True, 'contains(need)': True, 'contains(2)': True, 'contains(know)': True, 'contains(many)': True, 'contains(seat)': True, 'contains(next)': True, 'contains(put)': True, 'contains(us)': True, 'contains(standby)': True, 'contains(4)': True, 'contains(people)': True}            \n",
       "\n",
       "   1  \n",
       "0 -1  \n",
       "1  0  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
