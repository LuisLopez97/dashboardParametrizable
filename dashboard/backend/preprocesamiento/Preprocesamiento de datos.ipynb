{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento con Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el preprocesamiento de texto utilizaremos las librerías de Python:\n",
    "- **Numpy**: para el procesamiento de las operaciones en los Dataframes y Series de Pandas\n",
    "- **Pandas**: para la manipulación de los datos\n",
    "- **NLTK**: para el procesamiento de texto por medio de las StopWords, Stemming, Lemmatization y POS tag\n",
    "- **re**: filtrar datos con para expresiones regulares "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de datos con Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importacion de librerias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import emoji\n",
    "import seaborn as sns\n",
    "# Permite desplegar el texto completo en Jupyter\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.703060e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>No value</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>No Value</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>24/02/2015 11:35</td>\n",
       "      <td>No value</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>No value</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>No Value</td>\n",
       "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>24/02/2015 11:15</td>\n",
       "      <td>No value</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>No value</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>No Value</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>24/02/2015 11:15</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>No value</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>No Value</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;amp; they have little recourse</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>24/02/2015 11:15</td>\n",
       "      <td>No value</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>No value</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>No Value</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>24/02/2015 11:14</td>\n",
       "      <td>No value</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  5.703060e+17  neutral           1.0000                         \n",
       "1  5.703010e+17  positive          0.3486                         \n",
       "2  5.703010e+17  neutral           0.6837                         \n",
       "3  5.703010e+17  negative          1.0000                         \n",
       "4  5.703010e+17  negative          1.0000                         \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0  Can't Tell     0.0000                     Virgin America   \n",
       "1  Can't Tell     0.0000                     Virgin America   \n",
       "2  Can't Tell     0.0000                     Virgin America   \n",
       "3  Bad Flight     0.7033                     Virgin America   \n",
       "4  Can't Tell     1.0000                     Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  \\\n",
       "0  No value               cairdin     No Value             \n",
       "1  No value               jnardino    No Value             \n",
       "2  No value               yvonnalynn  No Value             \n",
       "3  No value               jnardino    No Value             \n",
       "4  No value               jnardino    No Value             \n",
       "\n",
       "                                                                                                                             text  \\\n",
       "0  @VirginAmerica What @dhepburn said.                                                                                              \n",
       "1  @VirginAmerica plus you've added commercials to the experience... tacky.                                                         \n",
       "2  @VirginAmerica I didn't today... Must mean I need to take another trip!                                                          \n",
       "3  @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse   \n",
       "4  @VirginAmerica and it's a really big bad thing about it                                                                          \n",
       "\n",
       "  tweet_coord     tweet_created tweet_location               user_timezone  \n",
       "0  [0.0, 0.0]  24/02/2015 11:35  No value       Eastern Time (US & Canada)  \n",
       "1  [0.0, 0.0]  24/02/2015 11:15  No value       Pacific Time (US & Canada)  \n",
       "2  [0.0, 0.0]  24/02/2015 11:15  Lets Play      Central Time (US & Canada)  \n",
       "3  [0.0, 0.0]  24/02/2015 11:15  No value       Pacific Time (US & Canada)  \n",
       "4  [0.0, 0.0]  24/02/2015 11:14  No value       Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lectura de CSV\n",
    "data = pd.read_csv(\"Tweets_pg_prepared.csv\")\n",
    "data.head(5) # Muestra los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remover URLs (Regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicación Regex\n",
    "1. **\\w+** : Uno o más carácteres alfanumericos\n",
    "2. **:\\/\\/** : Un \"://\"\n",
    "3. **\\S+**: uno o más carácteres que no sean espacios\n",
    "\n",
    "Explicación otro Regex\n",
    "1. **(http|https|ftp)**: Detectar si empieza con alguno de estos protocolos\n",
    "2. **://**: Seguido de un \"://\"\n",
    "3. **[a-zA-Z0-9\\\\./]**: E inmediatamente empieza una palabra seguido de un punto (.) una o mas veces (de esta manera se incluye el (.com y variantes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_noURL = data[\"text\"].str.replace('\\w+:\\/\\/\\S+',\"\")\n",
    "# Otro regex: (http|https|ftp)://[a-zA-Z0-9\\\\./]+\n",
    "#data_noURL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remover referencias (@Usernames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicación regex\n",
    "1. **@**: Si empieza con arroba (@)\n",
    "2. **(\\w+)**: y le sigue una o más palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_noUser = data_noURL.str.replace('@(\\w+)',\"\")\n",
    "#data_noUser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remover hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicación regex\n",
    "1. **#**: Si empieza con gato (#)\n",
    "2. **(\\w+)**: y le sigue una o más palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_noHashtag = data_noUser.str.replace('#(\\w+)',\"\")\n",
    "#data_noHashtag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reemplazar Contracciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario_contracciones = {\n",
    "        \"ain't\":\"is not\",\n",
    "        \"amn't\":\"am not\",\n",
    "        \"aren't\":\"are not\",\n",
    "        \"can't\":\"cannot\",\n",
    "        \"'cause\":\"because\",\n",
    "        \"couldn't\":\"could not\",\n",
    "        \"couldn't've\":\"could not have\",\n",
    "        \"could've\":\"could have\",\n",
    "        \"daren't\":\"dare not\",\n",
    "        \"daresn't\":\"dare not\",\n",
    "        \"dasn't\":\"dare not\",\n",
    "        \"didn't\":\"did not\",\n",
    "        \"doesn't\":\"does not\",\n",
    "        \"don't\":\"do not\",\n",
    "        \"e'er\":\"ever\",\n",
    "        \"em\":\"them\",\n",
    "        \"everyone's\":\"everyone is\",\n",
    "        \"finna\":\"fixing to\",\n",
    "        \"gimme\":\"give me\",\n",
    "        \"gonna\":\"going to\",\n",
    "        \"gon't\":\"go not\",\n",
    "        \"gotta\":\"got to\",\n",
    "        \"hadn't\":\"had not\",\n",
    "        \"hasn't\":\"has not\",\n",
    "        \"haven't\":\"have not\",\n",
    "        \"he'd\":\"he would\",\n",
    "        \"he'll\":\"he will\",\n",
    "        \"he's\":\"he is\",\n",
    "        \"he've\":\"he have\",\n",
    "        \"how'd\":\"how would\",\n",
    "        \"how'll\":\"how will\",\n",
    "        \"how're\":\"how are\",\n",
    "        \"how's\":\"how is\",\n",
    "        \"I'd\":\"I would\",\n",
    "        \"I'll\":\"I will\",\n",
    "        \"I'm\":\"I am\",\n",
    "        \"I'm'a\":\"I am about to\",\n",
    "        \"I'm'o\":\"I am going to\",\n",
    "        \"isn't\":\"is not\",\n",
    "        \"it'd\":\"it would\",\n",
    "        \"it'll\":\"it will\",\n",
    "        \"it's\":\"it is\",\n",
    "        \"I've\":\"I have\",\n",
    "        \"kinda\":\"kind of\",\n",
    "        \"let's\":\"let us\",\n",
    "        \"mayn't\":\"may not\",\n",
    "        \"may've\":\"may have\",\n",
    "        \"mightn't\":\"might not\",\n",
    "        \"might've\":\"might have\",\n",
    "        \"mustn't\":\"must not\",\n",
    "        \"mustn't've\":\"must not have\",\n",
    "        \"must've\":\"must have\",\n",
    "        \"needn't\":\"need not\",\n",
    "        \"ne'er\":\"never\",\n",
    "        \"o'\":\"of\",\n",
    "        \"o'er\":\"over\",\n",
    "        \"ol'\":\"old\",\n",
    "        \"oughtn't\":\"ought not\",\n",
    "        \"shalln't\":\"shall not\",\n",
    "        \"shan't\":\"shall not\",\n",
    "        \"she'd\":\"she would\",\n",
    "        \"she'll\":\"she will\",\n",
    "        \"she's\":\"she is\",\n",
    "        \"shouldn't\":\"should not\",\n",
    "        \"shouldn't've\":\"should not have\",\n",
    "        \"should've\":\"should have\",\n",
    "        \"somebody's\":\"somebody is\",\n",
    "        \"someone's\":\"someone is\",\n",
    "        \"something's\":\"something is\",\n",
    "        \"that'd\":\"that would\",\n",
    "        \"that'll\":\"that will\",\n",
    "        \"that're\":\"that are\",\n",
    "        \"that's\":\"that is\",\n",
    "        \"there'd\":\"there would\",\n",
    "        \"there'll\":\"there will\",\n",
    "        \"there're\":\"there are\",\n",
    "        \"there's\":\"there is\",\n",
    "        \"these're\":\"these are\",\n",
    "        \"they'd\":\"they would\",\n",
    "        \"they'll\":\"they will\",\n",
    "        \"they're\":\"they are\",\n",
    "        \"they've\":\"they have\",\n",
    "        \"this's\":\"this is\",\n",
    "        \"those're\":\"those are\",\n",
    "        \"'tis\":\"it is\",\n",
    "        \"'twas\":\"it was\",\n",
    "        \"wanna\":\"want to\",\n",
    "        \"wasn't\":\"was not\",\n",
    "        \"we'd\":\"we would\",\n",
    "        \"we'd've\":\"we would have\",\n",
    "        \"we'll\":\"we will\",\n",
    "        \"we're\":\"we are\",\n",
    "        \"weren't\":\"were not\",\n",
    "        \"we've\":\"we have\",\n",
    "        \"what'd\":\"what did\",\n",
    "        \"what'll\":\"what will\",\n",
    "        \"what're\":\"what are\",\n",
    "        \"what's\":\"what is\",\n",
    "        \"what've\":\"what have\",\n",
    "        \"when's\":\"when is\",\n",
    "        \"where'd\":\"where did\",\n",
    "        \"where're\":\"where are\",\n",
    "        \"where's\":\"where is\",\n",
    "        \"where've\":\"where have\",\n",
    "        \"which's\":\"which is\",\n",
    "        \"who'd\":\"who would\",\n",
    "        \"who'd've\":\"who would have\",\n",
    "        \"who'll\":\"who will\",\n",
    "        \"who're\":\"who are\",\n",
    "        \"who's\":\"who is\",\n",
    "        \"who've\":\"who have\",\n",
    "        \"why'd\":\"why did\",\n",
    "        \"why're\":\"why are\",\n",
    "        \"why's\":\"why is\",\n",
    "        \"won't\":\"will not\",\n",
    "        \"wouldn't\":\"would not\",\n",
    "        \"would've\":\"would have\",\n",
    "        \"y'all\":\"you all\",\n",
    "        \"you'd\":\"you would\",\n",
    "        \"you'll\":\"you will\",\n",
    "        \"you're\":\"you are\",\n",
    "        \"you've\":\"you have\",\n",
    "        \"Whatcha\":\"What are you\",\n",
    "        \"luv\":\"love\",\n",
    "        \"sux\":\"sucks\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando un conjunto de contracciones\n",
    "conjunto_contracciones = set(diccionario_contracciones.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traducir_contracciones(texto):\n",
    "    texto = texto.split(\" \")\n",
    "    j = 0\n",
    "    for palabra in texto:\n",
    "        # Checa si las palabras seleccionadas coinciden con el connjunto de emoticones\n",
    "        if palabra in conjunto_contracciones:\n",
    "            #print(\"Contraccion con \", palabra, \" a \", diccionario_contracciones[palabra], \" en \", texto)\n",
    "            # Si encuentra una coincidencia, la reemplaza con su respectiva traducción\n",
    "            texto[j] = diccionario_contracciones[palabra]\n",
    "        j = j + 1\n",
    "    # Retorna la cadena corregida\n",
    "    return ' '.join(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_noContracciones = data_noHashtag.str.replace(\"’\",\"'\")\n",
    "data_noContracciones = data_noContracciones.apply(lambda x: traducir_contracciones(x))\n",
    "#data_noContracciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamiento de emoticones y emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originalmente pensaba que algun analizador los podria detectar, pero despues de leer algunos artículos descubri que es mejor interpretarlos (convertirlos a palabras que expresen el sentimiento del emoticon). Esto es clave para medir la polaridad de un mensaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretación de emoticones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario_emoticones = {\n",
    "        \":)\":\"smiley\",\n",
    "        \":‑)\":\"smiley\",\n",
    "        \":-]\":\"smiley\",\n",
    "        \":-3\":\"smiley\",\n",
    "        \":->\":\"smiley\",\n",
    "        \"8-)\":\"smiley\",\n",
    "        \":-}\":\"smiley\",\n",
    "        \":)\":\"smiley\",\n",
    "        \":]\":\"smiley\",\n",
    "        \":3\":\"smiley\",\n",
    "        \":>\":\"smiley\",\n",
    "        \"8)\":\"smiley\",\n",
    "        \":}\":\"smiley\",\n",
    "        \":o)\":\"smiley\",\n",
    "        \":c)\":\"smiley\",\n",
    "        \":^)\":\"smiley\",\n",
    "        \"=]\":\"smiley\",\n",
    "        \"=)\":\"smiley\",\n",
    "        \":-))\":\"smiley\",\n",
    "        \":-D\":\"smiley\",\n",
    "        \"8‑D\":\"smiley\",\n",
    "        \"x‑D\":\"smiley\",\n",
    "        \"X‑D\":\"smiley\",\n",
    "        \":D\":\"smiley\",\n",
    "        \"8D\":\"smiley\",\n",
    "        \"xD\":\"smiley\",\n",
    "        \"XD\":\"smiley\",\n",
    "        \":-d\":\"smiley\",\n",
    "        \"8‑d\":\"smiley\",\n",
    "        \"x‑d\":\"smiley\",\n",
    "        \"X‑d\":\"smiley\",\n",
    "        \":d\":\"smiley\",\n",
    "        \"8d\":\"smiley\",\n",
    "        \"xd\":\"smiley\",\n",
    "        \"Xd\":\"smiley\",\n",
    "        \":‑(\":\"sad\",\n",
    "        \":‑c\":\"sad\",\n",
    "        \":‑<\":\"sad\",\n",
    "        \":‑[\":\"sad\",\n",
    "        \":(\":\"sad\",\n",
    "        \":c\":\"sad\",\n",
    "        \":<\":\"sad\",\n",
    "        \":[\":\"sad\",\n",
    "        \":-||\":\"sad\",\n",
    "        \">:[\":\"sad\",\n",
    "        \":{\":\"sad\",\n",
    "        \":@\":\"sad\",\n",
    "        \">:(\":\"sad\",\n",
    "        \":'‑(\":\"sad\",\n",
    "        \":'(\":\"sad\",\n",
    "        \":‑P\":\"playful\",\n",
    "        \"X‑P\":\"playful\",\n",
    "        \"x‑p\":\"playful\",\n",
    "        \":‑p\":\"playful\",\n",
    "        \":‑Þ\":\"playful\",\n",
    "        \":‑þ\":\"playful\",\n",
    "        \":‑b\":\"playful\",\n",
    "        \":P\":\"playful\",\n",
    "        \"XP\":\"playful\",\n",
    "        \"xp\":\"playful\",\n",
    "        \":p\":\"playful\",\n",
    "        \":Þ\":\"playful\",\n",
    "        \":þ\":\"playful\",\n",
    "        \":b\":\"playful\",\n",
    "        \";p\":\"playful\",\n",
    "        \"<3\":\"love\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando un conjunto de emoticones\n",
    "conjunto_emoticones = set(diccionario_emoticones.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traducir_emoticones(texto):\n",
    "    texto = texto.split(\" \")\n",
    "    j = 0\n",
    "    for palabra in texto:\n",
    "        # Checa si las palabras seleccionadas coinciden con el connjunto de emoticones\n",
    "        if palabra in conjunto_emoticones:\n",
    "            # Si encuentra una coincidencia, la reemplaza con su respectiva traducción\n",
    "            texto[j] = diccionario_emoticones[palabra]\n",
    "        j = j + 1\n",
    "    # Retorna la cadena corregida\n",
    "    return ' '.join(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_noEmoticones = data_noContracciones.apply(lambda x: traducir_emoticones(x))\n",
    "#data_noEmoticones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codigo para remover emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_noEmoji = data_noHashtag.str.replace(\"[\"\n",
    "#                            u\"\\U0001F600-\\U0001F64F\"  # emojis\n",
    "#                            u\"\\U0001F300-\\U0001F5FF\"  # simbolos & pictografos\n",
    "#                            u\"\\U0001F680-\\U0001F6FF\"  # simbolos de transporte y mapas\n",
    "#                            u\"\\U0001F1E0-\\U0001F1FF\"  # banderas (iOS)\n",
    "#                            u\"\\U00002702-\\U000027B0\"\n",
    "#                            u\"\\U000024C2-\\U0001F251\"\n",
    "#                            \"]+\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretación de emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_noEmojis = data_noEmoticones.apply(lambda x: emoji.demojize(x))\n",
    "data_noEmojis = data_noEmojis.str.replace(\":\",\" \")\n",
    "#data_noEmojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remover signos de puntuacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_noPunctuation = data_noEmojis.str.replace(\"[\\.\\,\\!\\?\\:\\;\\-\\=]\", \" \")\n",
    "data_noPunctuation = data_noPunctuation.str.replace(\" +\",\" \") # Reducir los espacios a solo 1\n",
    "#data_noPunctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertir mayúsculas a minúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lower = data_noPunctuation.str.lower() # Convertir todo el texto de la columna \"text\" a minusculas\n",
    "#data_lower "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretación de Slang (abreviaturas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scrapping de los acronimos de Netlingo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from bs4 import BeautifulSoup\n",
    "#import requests, json\n",
    "#resp = requests.get(\"http://www.netlingo.com/acronyms.php\")\n",
    "#soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "#slangdict = {}\n",
    "#key = \"\"\n",
    "#value = \"\"\n",
    "#for div in soup.findAll('div', attrs={'class':'list_box3'}):\n",
    "#    for li in div.findAll('li'):\n",
    "#        for a in li.findAll('a'):\n",
    "#            key = a.text\n",
    "#        value = li.text.split(key)[1]\n",
    "#        slangdict[key.upper()] = value\n",
    "#with open('myslang.json','w') as find:\n",
    "#    json.dump(slangdict, find, indent = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leer el archivo de Slang en JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descubrí que al tener un diccionario más amplio, afecta negativamente al análisis semántico porque traduce palabras como \"TIME\" a \"Tears In My Ears\" cuando en realidad el texto se refiere al \"Tiempo\". Así que no lo voy a usar, pero dejo el código para futuras referencias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slang = pd.read_json(\"myslang.json\", typ = \"series\")\n",
    "# # slang.to_frame('count') #para convertir a DataFrame\n",
    "# slang_df = slang.reset_index()\n",
    "# slang_np = slang_df[\"index\"].to_numpy()\n",
    "# slang_list = slang_np.tolist()\n",
    "# slang_set = set(slang_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import re\n",
    "# def translator(user_string):\n",
    "#     user_string = user_string.split(\" \")\n",
    "#     j = 0\n",
    "#     for _str in user_string:\n",
    "#         # Removiendo carácteres especiales\n",
    "#         #_str = re.sub('[^a-zA-Z0-9-_.]', '', _str)\n",
    "#         _str = _str.upper()\n",
    "#         # Checa si las palabras seleccionadas coinciden con las abreviaturas en el archivo de \"slang.txt\"\n",
    "#         if _str in slang_set:\n",
    "#             print(\"entro en \", user_string, \" con: \", _str, \" a \", slang[_str].lower())\n",
    "#             # Si encuentra una coincidencia, la reemplaza con su respectiva traducción\n",
    "#             user_string[j] = slang[_str].lower()\n",
    "#         j = j + 1\n",
    "#     # Retorna la cadena corregida\n",
    "#     return ' '.join(user_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_noSlang = data_lower.apply(lambda x: translator(x))\n",
    "# data_noSlang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Si los slangs hubieran estado en un TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de archivo\n",
    "slang_df = pd.read_csv(\"slang.txt\", sep = \"=\")\n",
    "slang_df.columns = [\"Slang\", \"Meaning\"]\n",
    "\n",
    "# Crear conjunto de Slangs\n",
    "slang_np = slang_df[\"Slang\"].to_numpy()\n",
    "slang_list = slang_np.tolist()\n",
    "slang_set = set(slang_list)\n",
    "\n",
    "# Hacer que la columna \"Slang\" sean los indices (para busquedas)\n",
    "slang_df = slang_df.set_index('Slang')\n",
    "slang = slang_df[\"Meaning\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traducir_slang(texto):\n",
    "    texto = texto.split(\" \")\n",
    "    j = 0\n",
    "    for palabra in texto:\n",
    "        palabra = palabra.upper()\n",
    "        # Checa si las palabras seleccionadas coinciden con el connjunto de emoticones\n",
    "        if palabra in slang_set:\n",
    "            #print(\"Slang en \", texto, \" con \", palabra, \" a \", slang[palabra])\n",
    "            # Si encuentra una coincidencia, la reemplaza con su respectiva traducción\n",
    "            texto[j] = slang[palabra].lower()\n",
    "        j = j + 1\n",
    "    # Retorna la cadena corregida\n",
    "    return ' '.join(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_noSlang = data_lower.apply(lambda x: traducir_slang(x))\n",
    "#data_noSlang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducción de carácteres repetidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como \"haaapppyyyy\" a \"haappyy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_noRepeated = data_noSlang.transform(lambda x: re.sub(r'(.)\\1+', r'\\1\\1', x))\n",
    "#data_noRepeated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remover StopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Son palabras que no aportan valor al analizar sentimientos, en Inglés serían palabras como \"are, you, have, etc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "#from nltk.corpus import stopwords\n",
    "stop = stopwords.words(\"english\")\n",
    "stop_set = set(stop)\n",
    "data_noStopwords = data_noRepeated.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_set)]))\n",
    "#data_noStopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming (volver a las palabras a su respectiva palabra raiz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen diferentes tipos de Stemmers, para el lenguaje Inglés,  podemos encontrar 2 de las más populares en la librería NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es conocido por su simplicidad y velocidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "data_PorterStemming = data_noStopwords.apply(lambda x: ' '.join([ps.stem(word) for word in x.split()]))\n",
    "#data_PorterStemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LancasterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es conocido por ser simple, pero tambien en ser muy duro al stemmizar, ya que realiza iteraciones y podría ocurrir una sobre-stemmización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = LancasterStemmer()\n",
    "data_LancasterStemming = data_noStopwords.apply(lambda x: ' '.join([ls.stem(word) for word in x.split()]))\n",
    "#data_LancasterStemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, ambos stemmers por si solos devuelven la cadena completa como si se tratara de una palabra:\n",
    "\n",
    "' plu ad commerc experience.. tacky.'\n",
    "\n",
    "Cuando debería ser:\n",
    "\n",
    "['plu' 'ad' 'commerc' 'experience' 'tacky']\n",
    "\n",
    "Para lograr ello realizamos una \"Tokenización\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\GIYELI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "#from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "def stemOracion(oracion):\n",
    "    token_words = word_tokenize(oracion)\n",
    "    token_words\n",
    "    stem_sentence = []\n",
    "    punctuations = \"?:!.,;\"\n",
    "    for word in token_words:\n",
    "        if word in punctuations:\n",
    "            token_words.remove(word)\n",
    "            continue\n",
    "        stem_sentence.append(ps.stem(word))\n",
    "    return stem_sentence\n",
    "\n",
    "\n",
    "porter_stemmer_tokenized = data_noStopwords.apply(lambda x: stemOracion(x))\n",
    "#porter_stemmer_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\GIYELI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "#from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "def stemOracion(oracion):\n",
    "    token_words = word_tokenize(oracion)\n",
    "    token_words\n",
    "    stem_sentence = []\n",
    "    punctuations = \"?:!.,;\"\n",
    "    for word in token_words:\n",
    "        if word in punctuations:\n",
    "            token_words.remove(word)\n",
    "            continue\n",
    "        stem_sentence.append(ls.stem(word))\n",
    "    return stem_sentence\n",
    "\n",
    "\n",
    "porter_stemmer_tokenized = data_noStopwords.apply(lambda x: stemOracion(x))\n",
    "#porter_stemmer_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization (es el Stemming pero con otro proceso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se desarrollara el Lemmatization para ver si con este proceso se obtienen mejores resultados. El siguiente código tambien incluye la Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\GIYELI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        say                                                                                                        \n",
       "1        plus add commercials experience tacky                                                                      \n",
       "2        today must mean need take another trip                                                                     \n",
       "3        really aggressive blast obnoxious & amp little recourse                                                    \n",
       "4        really big bad thing                                                                                       \n",
       "5        seriously would pay flight seat play really bad thing fly va                                               \n",
       "6        yes nearly every time fly vx worm away smiley                                                              \n",
       "7        really miss prime opportunity men without hat parody                                                       \n",
       "8        well didn't…but smiley                                                                                     \n",
       "9        amaze arrive hour early good                                                                               \n",
       "10       know suicide second lead cause death among teens 10 24                                                     \n",
       "11       & lt 3 pretty graphics much better minimal iconography smiley                                              \n",
       "12       great deal already think 2nd trip & amp even go 1st trip yet playful                                       \n",
       "13       fly sky take away travel                                                                                   \n",
       "14       thank                                                                                                      \n",
       "15       sfo pdx schedule still miss action                                                                         \n",
       "16       excite first cross country flight lax mco hear nothing great things virgin america                         \n",
       "17       fly nyc sfo last week could fully sit seat due two large gentleman either side help                        \n",
       "18       red_heart fly smiling_face thumbs_up                                                                       \n",
       "19       know would amazingly awesome bos fll please want fly                                                       \n",
       "20       first fare may three time carriers seat available select                                                   \n",
       "21       love graphic                                                                                               \n",
       "22       love hipster innovation feel good brand                                                                    \n",
       "23       make bos & gt las non stop permanently anytime soon                                                        \n",
       "24       guy mess seat reserve seat friends guy give seat away pouting_face want free internet                      \n",
       "25       status match program apply three weeks call email response                                                 \n",
       "26       happen 2 ur vegan food options least say ur site know able 2 eat anything next 6 hrs                       \n",
       "27       miss worry together soon                                                                                   \n",
       "28       amaze can not get cold air vent                                                                            \n",
       "29       lax ewr middle seat red eye noob maneuver                                                                  \n",
       "                           ...                                                                                      \n",
       "14610    understand weather issue can not expect passengers wait 24 hours inside airports whatever reason outrageous\n",
       "14611    guarantee retribution would glad share                                                                     \n",
       "14612    friend flight cancel flightlations lax cmh feb 23 anyway help 800 number help                              \n",
       "14613    use back operator regard flight get call 2 hours late flightr get hang                                     \n",
       "14614    need work tomorrow 8am therefore help direct message faster call 800 number                                \n",
       "14615    ugh dump us dfw w/no luggage cancel flight flight 3 time sit arrival tue                                   \n",
       "14616    cancel flight flight send email text call put way earlier flight might miss thank aa                       \n",
       "14617    dming big thank                                                                                            \n",
       "14618    3078 overweight pull 2 dozen passengers luggage seriously                                                  \n",
       "14619    love company staff amaze make uncomfortable situation comfortable                                          \n",
       "14620    wait 2+ hrs customer service call back flt cxld/protection & amp hang minute answer 1st ring               \n",
       "14621    hold 55 mins cancel flight international flight country can not leave call back # help                     \n",
       "14622    need place sleep land without accommodations pls                                                           \n",
       "14623    love new plan jfk lax run maybe one day one amenities function                                             \n",
       "14624    call chairman call emerald today call former customer                                                      \n",
       "14625    flight 236 great fantastic cabin crew a+ land                                                              \n",
       "14626    flight 953 nyc buenos air delay since yesterday 10pm go take 3 30pm give us answer                         \n",
       "14627    flight cancel flightled can not go home tomorrow could use dinner play first time nyc                      \n",
       "14628    thank relations review concern contact back directly john                                                  \n",
       "14629    change flight phone system keep tell representatives busy                                                  \n",
       "14630    thank                                                                                                      \n",
       "14631    thank nothing get us country back us break plane come get another one                                      \n",
       "14632    look good please follow link start refund process                                                          \n",
       "14633    flight cancel flightled leave tomorrow morning auto rebooked tuesday night flight need arrive monday       \n",
       "14634    right cue delay ok_hand                                                                                    \n",
       "14635    thank get different flight chicago                                                                         \n",
       "14636    leave 20 minutes late flight warn communication 15 minutes late flight that 's call shitty customer service\n",
       "14637    please bring american airlines                                                                             \n",
       "14638    money change flight answer phone suggestions make commitment                                               \n",
       "14639    8 people need 2 know many seat next flight please put us standby 4 people next flight                      \n",
       "Name: text, Length: 14640, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fuente: https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def lemmatization(oracion):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    punctuations = \"?:!.,;$\\\"\\'\\´\\``\\”\\“\\''\"\n",
    "    resultado = []\n",
    "    sentence_words = nltk.word_tokenize(oracion)\n",
    "    for word in sentence_words:\n",
    "        if word in punctuations:\n",
    "            sentence_words.remove(word)\n",
    "            continue\n",
    "        resultado.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "    return \" \".join(resultado)\n",
    "\n",
    "data_lemmatized = data_noStopwords.apply(lambda x: lemmatization(x))\n",
    "data_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Of Speech Tagging (POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sirve para etiquetar cada palabra en la oración como verbo, sustantivo o pronombre, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\GIYELI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#Fuente: https://towardsdatascience.com/basic-data-cleaning-engineering-session-twitter-sentiment-data-95e5bd2869ec\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "data_POS = data_noStopwords.apply(lambda x: nltk.pos_tag(nltk.word_tokenize(x)))\n",
    "#data_POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardar Lemmatization a CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertir los valores de Sentiment a 0 y 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = data[\"airline_sentiment\"].replace(to_replace=[\"positive\",\"neutral\",\"negative\"], value=[1,0,-1])\n",
    "#sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminar Columnas inecesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_sentiment = pd.DataFrame(dict(data_lemmatized = data_lemmatized, sentiment = sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_csv = bag_sentiment.to_csv (r'data_lemmatized.csv', index = None, header=True) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
